# -*- coding: utf-8 -*-
"""Big_Data_Clustering_Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QUYNNPhe8iW0fVkTb4rsfrGIXVToiCS0

# Big Data Analysis Project

Dataset: Online Retail Transactions

This notebook covers preprocessing, EDA, big data processing with Dask, and clustering.
"""

# Install required libraries (if needed)
!pip install dask[complete] scikit-learn matplotlib seaborn

import dask.dataframe as dd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

"""## Data Loading"""

df = dd.read_csv('/content/data.csv', encoding='cp1252', blocksize=None, dtype={'CustomerID': 'float64', 'InvoiceNo': 'object', 'Quantity': 'float64'})
df.head()

"""## Data Preprocessing"""

# Remove duplicates
df = df.drop_duplicates()

# Handle missing values
df = df.dropna(subset=['CustomerID'])

# Convert InvoiceDate to datetime
df['InvoiceDate'] = dd.to_datetime(df['InvoiceDate'])

"""## Feature Engineering (RFM)"""

# Compute RFM features
snapshot_date = df['InvoiceDate'].max().compute()

# Calculate Recency (days since last purchase)
last_purchase = df.groupby('CustomerID')['InvoiceDate'].max()
recency_df = (snapshot_date - last_purchase).dt.days.to_frame(name='Recency').compute()

# Calculate Frequency (number of unique invoices per customer)
frequency_df = df.groupby('CustomerID')['InvoiceNo'].nunique().to_frame(name='Frequency').compute()

# Calculate Monetary (sum of UnitPrice for each customer)
# First, calculate TotalPrice for each transaction
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']
monetary_df = df.groupby('CustomerID')['TotalPrice'].sum().to_frame(name='Monetary').compute()

# Combine the RFM features into a single pandas DataFrame
rfm = recency_df.join(frequency_df).join(monetary_df)

rfm.head()



"""## Clustering"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(
    rfm[['Recency', 'Frequency', 'Monetary']]
)

kmeans = KMeans(n_clusters=3, random_state=42)
rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)

"""## Evaluation"""

rfm_clean = rfm[['Recency', 'Frequency', 'Monetary']].dropna()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_clean)

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(rfm_scaled)

from sklearn.metrics import silhouette_score

score = silhouette_score(rfm_scaled, clusters)
print("Silhouette Score:", score)

"""## Visualization"""

rfm['Cluster'].value_counts().plot(kind='bar')
plt.title('Customer Segments')
plt.show()

rfm['Cluster'].value_counts().sort_index().plot(kind='bar')
plt.xlabel('Cluster')
plt.ylabel('Jumlah Pelanggan')
plt.title('Distribusi Pelanggan per Cluster')
plt.show()

inertia = []
K = range(1, 8)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm_scaled)
    inertia.append(kmeans.inertia_)

plt.plot(K, inertia, marker='o')
plt.xlabel('Jumlah Cluster')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

cluster_names = {
    0: 'Low Value Customers',
    1: 'Medium Value Customers',
    2: 'High Value Customers'
}

rfm['Cluster_Name'] = rfm['Cluster'].map(cluster_names)
rfm.head()